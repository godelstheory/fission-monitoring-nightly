---
title: "Fission Monitoring Nightly: Analysis ETL"
author: "Corey Dow-Hygelund, Mozilla Data Science"
date: "`r Sys.Date()`"
output:
  html_document:
    mathjax: null
    toc: true
    toc_collapsed: false
    toc_depth: 5
    number_sections: true
    theme: cosmo
params:
    args: !r list()

---

<style>
body {
    line-height: 1.4em;
    width: 100%;
    }
.plotly {
    text-align: center;
    width: 75vw;
    position: relative;
    margin-left: calc((100% - 75vw)/2);
}
.zimg img {
    text-align: center;
    width: 75vw;
    position: relative;
    margin-left: calc((100% - 75vw)/2);
}
.r {
    background-color: white;
    border: 0;
        }
        
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
.caption {
    font-size: 80%;
    font-style: italic;
    font-weight:bold;
        }

caption {
    font-size: 80%;
    font-style: italic;
    font-weight: bold;
        }

h3, .h3 {
    margin-top: 100px;
    }
    

</style>


```{r sources}
source('params.R')
source('query.R')
source('stats.R')
```


```{r imports}
library(bigrquery)
library(dlyr)
library(data.table)
```

```{r helpers}
##FIXME: Move these that persist after pipeline is finalized 

# Single clients from older builds in Nightly dev table
remove_single_builds <- function(df) {
  bunk_builds <- df %>% 
    group_by(build_id) %>% 
    summarize(nbranches = length(unique(branch)), .groups = 'drop') %>% 
    filter(nbranches < 2)
  return(
    df %>% 
      filter(!build_id %in% bunk_builds$build_id)
  )
}
```

```{r main_import}
options(scipen = 20) # bigrquery bug: https://github.com/r-dbi/bigrquery/issues/395 

# main_query <- build_main_query(probes.hist, slug, main_tbl)
# main <- bq_project_query(project_id, main_query)
# main.df <- bq_table_download(main)
```

```{r crashes_import}
# place_holder
```

# Filters

Query for previously processed builds. Apply this as a query filter to retrieve only new, unprocessed, builds. 

# Histogram Aggregation

**TODO** There are several methods necessary for handling histograms. This will require a map of probe_name -> hist_agg_method. Defaulting to `summarize.hist` for the moment. 


```{r hist_agg}
results.hist <- list()
hists.raw <- list()  #TODO: For initial EDA: remove after histograms well-understood
print('Processing histograms')
for(probe in names(probes.hist)){
  if (probe %in% results.hist){
    print(probe)
    hist_query <- build_hist_query(probes.hist[[probe]], slug, main_tbl)
    hist <- bq_project_query(project_id, hist_query)
    hist.df <- bq_table_download(hist) %>%
      remove_single_builds() %>%  #FIXME: Hack for dev table support
      as.data.table()
    
    results.hist[[probe]] <- summarize.hist(hist.df) %>% 
      mutate(probe = probe) %>%
      mutate(probe = probe) %>%
      rename(branch = what)
    
    hists.raw[[probe]] <- hist.df
  }
}

save(hists.raw, results.hist, file='hists_raw.df')
```

# Scalar Aggregation


# Export

Combine the individual probes into a single `data.frame`

```{r combine}
results.hist.df <- rbindlist(results.hist) %>%
  mutate(date_computed = Sys.Date())

final.df <- results.hist.df
```

Export finalized dataset to BigQuery for display in dashboard
**FIXME**: Temporarily using `moz-fx-data-bq-data-science.cdowhygelund.fission_monitoring_analyzed_v1` until credential issue is solved.

```{r export}
# bigrquery method: not sure how to configure credentials to write to `moz-fx-data-shared-prod`
bq_table(project = project_id,
         # dataset = "analysis",
         dataset = 'cdowhygelund',
         table   = "fission_monitoring_analyzed_v1") %>%
  bq_table_upload(values = results.hist.df,
                  create_disposition = "CREATE_IF_NEEDED",
                  write_disposition = "WRITE_APPEND",
                  fields = as_bq_fields(final.df) )


# sguha method using command line: might be necessary to circumvent credentials issue

# atemp <- tempfile()
# fwrite(final.df, file=atemp,row.names=FALSE,quote=TRUE,na=0)
# l <- glue("bq load --noreplace   --project_id='moz-fx-data-shared-prod'  --source_format=CSV --skip_leading_rows=1 --null_marker=NA",
#           " \"{generate_shell_export_tblname(tbl.analyzed)}\" {atemp} ./tbl_analyzed_fields.json")
# #loginfo(l)
# system(l)
```


# TODO

* Query for previous max_build_id and limit initial import to those unprocessed. 
* Limit build ID window to 10 days to reduce processing. 
* Delete from the analysis table anything that has a buildID that is within `final.df`

* Add total number of unique users per branch per build
* should "MEMORY_TOTAL" = 'payload.processes.histograms.memory_total' and not content?

# FIXME

* Exported table should be in `moz-fx-data-shared-prod`. Issues regarding `bigrquery` and credentials. 

* `FX_NUMBER_OF_UNIQUE_SITE_ORIGINS_PER_LOADED_TABS` is a composition of many histograms. Only using `payload.histograms.fx_number_of_unique_site_origins_per_loaded_tabs_1` for POC. 
