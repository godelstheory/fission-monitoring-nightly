---
title: "Fission Monitoring Nightly: Analysis ETL"
author: "Corey Dow-Hygelund, Mozilla Data Science"
date: "`r Sys.Date()`"
output:
  html_document:
    mathjax: null
    toc: true
    toc_collapsed: false
    toc_depth: 5
    number_sections: true
    theme: cosmo
params:
    args: !r list()

---

<style>
body {
    line-height: 1.4em;
    width: 100%;
    }
.plotly {
    text-align: center;
    width: 75vw;
    position: relative;
    margin-left: calc((100% - 75vw)/2);
}
.zimg img {
    text-align: center;
    width: 75vw;
    position: relative;
    margin-left: calc((100% - 75vw)/2);
}
.r {
    background-color: white;
    border: 0;
        }
        
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
.caption {
    font-size: 80%;
    font-style: italic;
    font-weight:bold;
        }

caption {
    font-size: 80%;
    font-style: italic;
    font-weight: bold;
        }

h3, .h3 {
    margin-top: 100px;
    }
    

</style>


```{r sources}
source('params.R')
source('query.R')
source('stats.R')
```


```{r imports}
library(bigrquery)
library(data.table)
library(dplyr)
```

```{r args}
max_build_id <- '20200908' #TODO: Define from query and as supplied arg
```



```{r main_import}
options(scipen = 20) # bigrquery bug: https://github.com/r-dbi/bigrquery/issues/395 

# main_query <- build_main_query(probes.hist, slug, main_tbl)
# main <- bq_project_query(project_id, main_query)
# main.df <- bq_table_download(main)
```

```{r crashes_import}
# place_holder
```

# Filters

Query for previously processed builds. Apply this as a query filter to retrieve only new, unprocessed, builds. 

# Histogram Aggregation

Perform the histogram aggregation server-side. This is most easy achieved processing a histogram at a time. 

**TODO** There are several methods necessary for handling histograms. This will require a map of probe_name -> hist_agg_method. Defaulting to `summarize.hist` for the moment. 


```{r hist_agg_var}
results.hist <- list()
hists.raw <- list()  #TODO: For initial EDA: remove after histograms well-understood
```

```{r hist_agg}
print('Processing histograms')
for(probe in names(probes.hist)){
    print(probe)
    hist_query <- build_hist_query(probes.hist[[probe]], slug, main_tbl, max_build_id)
    hist <- bq_project_query(project_id, hist_query)
    hist.df <- bq_table_download(hist) %>%
      as.data.table()
    
    results.hist[[probe]] <- summarize.hist(hist.df) %>% 
      mutate(probe = probe) %>%
      rename(branch = what)
    
    hists.raw[[probe]] <- hist.df
}

save(hists.raw, results.hist, file='hists_raw.2df')
```



# Scalar Aggregation

Pull the each's build per daily average of the scalars. 
```{r scalar_agg}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.mean, slug, main_tbl, max_build_id))
scalar.df <- bq_table_download(scalar) %>%
  remove_single_builds() %>%
  as.data.table()
```
```{r scalar_agg_var}
results.scalar <- list()
```

```{r scalar_agg, warning=FALSE}
bs_replicates <- 20 #TODO: Debugging

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max))){
# for (probe in c(names(probes.scalar.max))){
  print(probe)
  results.scalar[[probe]] <- scalar.df[get(probe)<quantile(get(probe), 0.999, na.rm = TRUE),
                                  summarize.scalar(.SD[,list(id,branch,x=get(probe))], "x", bs_replicates, stat=mean.narm),
                                  by=build_id][, probe := probe][order(build_id, what),] %>%
                              rename(branch = what)
}
```


# Export

Combine the individual probes into a single `data.frame`

```{r combine}
final.df <- rbindlist(results.hist) %>%
  rbind(., rbindlist(results.scalar)) %>%
  mutate(date_computed = Sys.Date())

```

Export finalized dataset to BigQuery for display in dashboard
**FIXME**: Temporarily using `moz-fx-data-bq-data-science.cdowhygelund.fission_monitoring_analyzed_v1` until credential issue is solved.

```{r export}
# bigrquery method: not sure how to configure credentials to write to `moz-fx-data-shared-prod`
bq_table(project = project_id,
         # dataset = "analysis",
         dataset = 'cdowhygelund',
         table   = "fission_monitoring_analyzed_v1") %>%
  bq_table_upload(values = final.df,
                  create_disposition = "CREATE_IF_NEEDED",
                  write_disposition = "WRITE_APPEND",
                  fields = as_bq_fields(final.df) )


# sguha method using command line: might be necessary to circumvent credentials issue

# atemp <- tempfile()
# fwrite(final.df, file=atemp,row.names=FALSE,quote=TRUE,na=0)
# l <- glue("bq load --noreplace   --project_id='moz-fx-data-shared-prod'  --source_format=CSV --skip_leading_rows=1 --null_marker=NA",
#           " \"{generate_shell_export_tblname(tbl.analyzed)}\" {atemp} ./tbl_analyzed_fields.json")
# #loginfo(l)
# system(l)
```


# TODO

* Query for previous max_build_id and limit initial import to those unprocessed. 
* Limit build ID window to 10 days to reduce processing. 
* Delete from the analysis table anything that has a buildID that is within `final.df`
* For scalar query, perform dense_rank on client_id to get `id` field
* Scalar boostrapping should all be done with same replicates. Statistically more sound, and MUCH faster. 

* Missing Analysis Fields
  - Add total number of unique users per branch per build
  - 
* should "MEMORY_TOTAL" = 'payload.processes.histograms.memory_total' and not content?
* Purpose of: sum(coalesce(VALUE_SUM(checkerboard_severity,500))) 
* Handling `device_resets`?


# FIXME
* Exported table should be in `moz-fx-data-shared-prod`. Issues regarding `bigrquery` and credentials. 

* `FX_NUMBER_OF_UNIQUE_SITE_ORIGINS_PER_LOADED_TABS` is a composition of many histograms. Only using `payload.histograms.fx_number_of_unique_site_origins_per_loaded_tabs_1` for POC. 
